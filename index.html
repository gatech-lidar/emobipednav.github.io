<!DOCTYPE html>
<html lang="en">
  <head>
      <meta charset="UTF-8">
      <link rel="stylesheet" href="main.css">
      <link rel="icon" type="image/x-icon" href="assets/digit.png">
      <title>EmoBipedNav</title>
  </head>
  <body>

   
      
     


    <div id="title_slide">
        <div class="title_left">
            <h1>EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots <br> with Deep Reinforcement Learning</h1>
        

            <div class="box alt">
              <div class="row uniform">
                <div class="2u">
                  <a href="https://www.weizhu996.com/">                                
                    <span class="image fit"><img src="assets/author_photo/weizhu.png" alt="Wei Zhu" /></span>Wei Zhu<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://abirathr.github.io/AbirathRaju.github.io/">         
                    <span class="image fit"><img src="assets/author_photo/abirath.jpg" alt="Abirath Raju" /></span>Abirath Raju<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://scholar.google.com/citations?user=41GA5O4AAAAJ&hl=en">     
                    <span class="image fit"><img src="assets/author_photo/aziz_shamsah.png" alt="Abdulaziz Shamsah" /></span>Abdulaziz Shamsah<sup>2</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://sites.google.com/site/anqiwuresearch/">                 
                    <span class="image fit"><img src="assets/author_photo/anqi.jpg" alt="Anqi Wu" /></span>Anqi Wu<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://scholar.google.com/citations?user=-JPZ21IAAAAJ&hl=en">                 
                    <span class="image fit"><img src="assets/author_photo/seth.png" alt="Seth Hutchinson" /></span>Seth Hutchinson<sup>3</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://research.gatech.edu/people/ye-zhao">   
                    <span class="image fit"><img src="assets/author_photo/ye_zhao.jpg" alt="Ye Zhao" /></span>Ye Zhao<sup>1</sup>
                  </a>
                </div>
              </div>
          </div>

            <div class="gatech">
                <p><sup>1</sup>Georgia Institute of Technology, <sup>2</sup>Kuwait University, <sup>3</sup>Northeastern University</p>
            </div>
            <div class="button-container">
                <a href="https://arxiv.org/abs/2409.20514" class="button">Paper</a>
                <a href="https://youtu.be/fNNL56sTSjY?si=sLgWn5P6o4MndMzl" class="button">Video</a>
                <a href="https://github.com/GTLIDAR/emobipednav" class="code">Video</a>
            </div>

            <br>

            <div id="abstract" class="grid-container">
                <p>
                  This study presents an emotion-aware navigation framework -- EmoBipedNav -- using deep reinforcement learning (DRL) 
                  for bipedal robots walking in socially interactive environments. The inherent locomotion constraints of bipedal robots 
                  challenge their safe maneuvering capabilities in dynamic environments. When combined with the intricacies of social 
                  environments, including pedestrian interactions and social cues, such as emotions, these challenges become even more 
                  pronounced. To address these coupled problems, we propose a two-stage pipeline that considers both bipedal locomotion 
                  constraints and complex social environments. Specifically, social navigation scenarios are represented using sequential 
                  LiDAR grid maps (LGMs), from which we extract latent features, including collision regions, emotion-related discomfort 
                  zones, social interactions, and the spatio-temporal dynamics of evolving environments. The extracted features are 
                  directly mapped to the actions of reduced-order models (ROMs) through a DRL architecture. Furthermore, the proposed 
                  framework incorporates full-order dynamics and locomotion constraints during training, effectively accounting for 
                  tracking errors and restrictions of the locomotion controller while planning the trajectory with ROMs. Comprehensive 
                  experiments demonstrate that our approach exceeds both model-based planners and DRL-based baselines.
                </p>
            </div>
        </div>
    </div>
    <hr class="rounded">
    
    <div id="overview">

        <h1>EmoBipedNav</h1>

        <p>
          Our framework begins by obtaining estimated facial emotions using pre-trained CNN models. Simultaneously, we transform raw LiDAR 
          scans into sequential pie-shape LGMs. These grid maps are converted into stacked pixel images which are further processed through 
          an encoder constructed using convolutional neural networks (CNNs) to extract socially interactive and emotionally aware features. 
          The resulting latent features are concatenated with the robot's last command and target position, which are fed into an actor-critic 
          DRL structure implemented with multi-layer perceptrons (MLPs). The action output from the actor network is derived from the 
          reduce-order model (ROM) and practically applied to a bipedal robot with full-body dynamics and constraints. The torso position and 
          yaw angle obtained from Digit correspond to the ego-agent state. We use the angular momentum linear inverted pendulum 
          planner (ALIP) and a passivity full-body controller with ankle actuation to track the desirable ROM trajectory.
        </p>

        <div class="approach">
          <div class="video_container">
              <iframe width="100%" height="100%" src="https://youtu.be/uOsFafqcxEs" 
                      title="approach intruduction" frameborder="0" 
                      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                      allowfullscreen>
              </iframe>
          </div>
        </div>


        <h1>Simulation benchmark in MuJoCo</h1>
        <p>
          Our approach leverages differential dynamic programming (DDP) to generate whole-body motions that obey the robot's dynamics and task requirements.
        </p>
        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/TO.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <p>
          We study a rich set of loco-manipulation tasks, including walking, stair traversing, agile jumping, desk object reaching, and bulky-object handling.
          For box manipulation tasks, we design heuristic target goals for the robot's hand trajectory, accounting for the transition between contact phases with and without holding the box by adding or removing its mass and inertia from the hands.
        </p>
        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/ref_motion.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <h1>Training in MuJoCo</h1>

        <p>
          We train RL-based motion imitation policies in MuJoCo simulator.
        </p>
        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/training_mujoco.mp4" type="video/mp4">
                </video>
            </div>
        </div>


        <h1>Compare Pure RL with Opt2Skill</h1>

        <p>
          We compare Opt2Skill with a pure RL method that learns from scratch in three different tasks, i.e., <i>Walking</i>, <i>Box Pickup</i>, and <i>Desk Object Reaching</i>. 
          The pure RL baseline excludes reference trajectories in the observation space and the reward design.
        </p>
        <p>
          Opt2Skill shows more accurate tracking, particularly in velocity for <i>Walking</i> and end-effector positioning for <i>Box Pickup</i> and <i>Desk Object Reaching</i>. 
          In contrast, pure RL exhibits larger deviations in velocity, fails to lift the box, and could not accurately reach the target object in the <i>Desk Object Reaching</i> task.
        </p>
        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/baseline.mp4" type="video/mp4">
                </video>
            </div>
        </div>


        <h1>Torque Limits and Tracking Impact</h1>

  
        <p>
          We demenstrate that both including the torque limit in the reference trajectory and incorporating the torque reference into reward design significantly contribute to motion tracking performance. 
          The torque limit ensures a high-quality and dynamically feasible reference trajectory, while the torque reference guides the robot to track the motion more precisely.
        </p>
        <div class="allegrofail">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata">
                    <source src="assets/ablation_TO.mp4" type="video/mp4">
                </video>
            </div>
        </div>


        

       
        <h1>BibTeX</h1>
         <p class="bibtex">
            @article{liu2024opt2skill,<br>
            &nbsp;&nbsp;title={Opt2Skill: Imitating Dynamically-feasible Whole-Body Trajectories for Versatile Humanoid Loco-Manipulation},<br>
            &nbsp;&nbsp;author={Liu, Fukang and Gu, Zhaoyuan and Cai, Yilin and Zhou, Ziyi and Zhao, Shijie and Jung, Hyunyoung and Ha, Sehoon and Chen, Yue and Xu, Danfei and Zhao, Ye},<br>
            &nbsp;&nbsp;journal={arXiv preprint arXiv:2409.20514}<br>
            &nbsp;&nbsp;year={2024},<br>
            }
        </p>

        <h1>Acknowledgements</h1>
        <p>
          The authors would like to thank all the developers of <a href="https://github.com/loco-3d/crocoddyl" target="_blank" style="color: blue;">Crocoddyl</a> 
          and <a href="https://github.com/stack-of-tasks/pinocchio" target="_blank" style="color: blue;">Pinocchio</a> for their open source frameworks. 
          We are especially grateful to <a href="https://zhaomingxie.github.io/" target="_blank" style="color: blue;">Zhaoming Xie</a> and <a href="https://cmastalli.github.io/" target="_blank" style="color: blue;">Carlos Mastalli</a> for their professional discussions and insightful feedback.
        </p>
       

        <div class="footer">
          <p>This website was developed based on <a href="https://github.com/learning-humanoid-locomotion/learning-humanoid-locomotion.github.io" target="_blank">learning-humanoid-locomotion</a></p>
      </div>
      
    </div>
    <script type="text/javascript">
        /* https://stackoverflow.com/questions/3027707/how-to-change-the-playing-speed-of-videos-in-html5 */
        document.querySelector('video').defaultPlaybackRate = 1.0;
        document.querySelector('video').play();

        var videos =document.querySelectorAll('video');
        for (var i=0;i<1;i++)
        {
            videos[i].playbackRate = 1.0;
        }
    </script>
    <script>
        /* https://stackoverflow.com/questions/21163756/html5-and-javascript-to-play-videos-only-when-visible */
        var videos = document.getElementsByTagName("video");

        function checkScroll() {
            var fraction = 0.5; // Play when 70% of the player is visible.

            for(var i = 0; i < 1; i++) {  // only apply to the first video

                var video = videos[i];

                var x = video.offsetLeft, y = video.offsetTop, w = video.offsetWidth, h = video.offsetHeight, r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }
        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Function to check if the user is on a mobile device
            function isMobileDevice() {
                return /Mobi|Android/i.test(navigator.userAgent);
            }
            // If the user is on a mobile device, disable autoplay
            if (isMobileDevice()) {
                const videos = document.querySelectorAll('video');
                videos.forEach(video => {
                    video.autoplay = false;
                    video.controls = true;
                });
            }
        });
    </script>
    <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
            type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
            crossorigin="anonymous"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
            type="text/javascript"></script>
  </body>

</html>
