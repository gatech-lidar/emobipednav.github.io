<!DOCTYPE html>
<html lang="en">
  <head>
      <meta charset="UTF-8">
      <link rel="stylesheet" href="main.css">
      <link rel="icon" type="image/x-icon" href="assets/digit.png">
      <title>EmoBipedNav</title>
  </head>
  <body>


    <div id="title_slide">
        <div class="title_left">
            <h1>EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots <br> with Deep Reinforcement Learning</h1>
        

            <div class="box alt">
              <div class="row uniform">
                <div class="2u">
                  <a href="https://www.weizhu996.com/">                                
                    <span class="image fit"><img src="assets/author_photo/weizhu.png" alt="Wei Zhu" /></span>Wei Zhu<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://abirathr.github.io/AbirathRaju.github.io/">         
                    <span class="image fit"><img src="assets/author_photo/abirath.jpg" alt="Abirath Raju" /></span>Abirath Raju<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://scholar.google.com/citations?user=41GA5O4AAAAJ&hl=en">     
                    <span class="image fit"><img src="assets/author_photo/aziz_shamsah.png" alt="Abdulaziz Shamsah" /></span>Abdulaziz Shamsah<sup>2</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://sites.google.com/site/anqiwuresearch/">                 
                    <span class="image fit"><img src="assets/author_photo/anqi.jpg" alt="Anqi Wu" /></span>Anqi Wu<sup>1</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://scholar.google.com/citations?user=-JPZ21IAAAAJ&hl=en">                 
                    <span class="image fit"><img src="assets/author_photo/seth.png" alt="Seth Hutchinson" /></span>Seth Hutchinson<sup>3</sup>
                  </a>
                </div>
                <div class="2u">
                  <a href="https://research.gatech.edu/people/ye-zhao">   
                    <span class="image fit"><img src="assets/author_photo/ye_zhao.jpg" alt="Ye Zhao" /></span>Ye Zhao<sup>1</sup>
                  </a>
                </div>
              </div>
          </div>

            <div class="gatech">
                <p><sup>1</sup>Georgia Institute of Technology, <sup>2</sup>Kuwait University, <sup>3</sup>Northeastern University</p>
            </div>
            <div class="button-container">
                <a href="http://arxiv.org/abs/2503.12538" class="button">Paper</a>
                <a href="https://youtu.be/fNNL56sTSjY?si=sLgWn5P6o4MndMzl" class="button">Video</a>
                <a href="https://github.com/GTLIDAR/emobipednav" class="button">Code</a>
            </div>

            <br>

            <div id="abstract" class="grid-container">
                <p>
                  This study presents an emotion-aware navigation framework -- EmoBipedNav -- using deep reinforcement learning (DRL) 
                  for bipedal robots walking in socially interactive environments. The inherent locomotion constraints of bipedal robots 
                  challenge their safe maneuvering capabilities in dynamic environments. When combined with the intricacies of social 
                  environments, including pedestrian interactions and social cues, such as emotions, these challenges become even more 
                  pronounced. To address these coupled problems, we propose a two-stage pipeline that considers both bipedal locomotion 
                  constraints and complex social environments. Specifically, social navigation scenarios are represented using sequential 
                  LiDAR grid maps (LGMs), from which we extract latent features, including collision regions, emotion-related discomfort 
                  zones, social interactions, and the spatio-temporal dynamics of evolving environments. The extracted features are 
                  directly mapped to the actions of reduced-order models (ROMs) through a DRL architecture. Furthermore, the proposed 
                  framework incorporates full-order dynamics and locomotion constraints during training, effectively accounting for 
                  tracking errors and restrictions of the locomotion controller while planning the trajectory with ROMs. Comprehensive 
                  experiments demonstrate that our approach exceeds both model-based planners and DRL-based baselines.
                </p>
            </div>
        </div>
    </div>
    <hr class="rounded">
    
    <div id="overview">

        <h1>EmoBipedNav</h1>

        <p>
          Our framework begins by obtaining estimated facial emotions using pre-trained CNN models. Simultaneously, we transform raw LiDAR 
          scans into sequential pie-shape lidar grid maps (LGMs). These grid maps are converted into stacked pixel images which are further 
          processed through an encoder constructed using convolutional neural networks (CNNs) to extract socially interactive and emotionally 
          aware features. The resulting latent features are concatenated with the robot's last command and target position, which are fed 
          into an actor-critic deep reinforcement learning (DRL) structure implemented with multi-layer perceptrons (MLPs). The action output 
          from the actor network is derived from the reduce-order model (ROM) and practically applied to a bipedal robot with full-body 
          dynamics and constraints. The torso position and yaw angle obtained from Digit correspond to the ego-agent state. We use the angular 
          momentum linear inverted pendulum planner (ALIP) and a passivity full-body controller with ankle actuation to track the desirable 
          ROM trajectory.
        </p>

        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/uOsFafqcxEs?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Simulation benchmark in MuJoCo</h1>
        <p>
          ​Initially, we train and test our navigation policy in a physics-based simulator MuJoCo, using a full-body bipedal robot and a full-order 
          dynamics. Simulatioin comparisons demonstrate the effectiveness of our social navigation pipeline for bipedal robots. 
        </p>

        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/DsfEIVZ708A?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Simulation in complex scenario</h1>
        <p>
          We deploy our policy into a more complex simulation scenarios, further indicating that our navigation policy enables the bipedal 
          robot to adapt to complex and interactive scenarios integrated with pedestrian emotions.
        </p>

        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/IpVLqscL89o?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Sim-to-real</h1>
        <p>
          We transfer our simulated navigation policy into real-world scenarios with diverse pedestrian motion patterns. The robot should avoid 
          static obstacles while interacting with pedestrians. We include several representative motion patterns. For example, pedestrians 
          cross in front of the robot, walk and suddenly stop in front of the robot, group together to cross in front of the robot, and randomly 
          interact with the robot and other pedestrians. Furthermore, we randomly set pedestrian emotions. Accordingly, our social navigation 
          policy can well adapt to its own behaviors to avoid collisions and reduce intrusions into discomfort zones of pedestrians, indicating 
          the potential of sim-to-real transfer of our navigation policy. In addition, we integrate robot localization, pedestrian detection, 
          and emotion recognition into our pipeline to further validate the practicality of our emotion-aware social navigation. These modules 
          are achieved by an on-board stereo camera, and corresponding SLAM and perception algorithms. Pedestrians’ states can change between 
          static and moving, and their emotions also alter between happy, neutral, and negative. To investigate further, we increase the 
          environment complexities. For example, pedestrians group together and keep static to block the robot. Moreover, we increase the 
          pedestrian density to make the scenario more crowded. 
        </p>
        
        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/KnCzS2p8oLc?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>

        <h1>Outdoor experiments</h1>
        <p>
          Finally, we deploy our navigation policy into more realistic environments, like the campus after classes. Pedestrians are more 
          crowded, and interactions are more complicated. The bipedal robot can still achieve the emotion-aware social navigation task.
        </p>

        <div style="display: flex; justify-content: center;">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/QGdr_MnXXw0?autoplay=1&mute=1" 
            frameborder="0" 
            allow="autoplay; encrypted-media" 
            allowfullscreen>
          </iframe>
        </div>
       
        <h1>BibTeX</h1>
         <p class="bibtex">
            @article{zhu2025emobipednav,<br>
            &nbsp;&nbsp;title={EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with Deep Reinforcement Learning},<br>
            &nbsp;&nbsp;author={Wei Zhu, Abirath Raju, Abdulaziz Shamsah, Anqi Wu, Seth Hutchinson, and Ye Zhao},<br>
            &nbsp;&nbsp;journal={arXiv preprint arXiv:2503.12538}<br>
            &nbsp;&nbsp;year={2025},<br>
            }
        </p>

      
       

        <div class="footer">
          <p>This website was developed based on <a href="https://github.com/learning-humanoid-locomotion/learning-humanoid-locomotion.github.io" target="_blank">learning-humanoid-locomotion</a></p>
      </div>
      
    </div>
    <script type="text/javascript">
        /* https://stackoverflow.com/questions/3027707/how-to-change-the-playing-speed-of-videos-in-html5 */
        document.querySelector('video').defaultPlaybackRate = 1.0;
        document.querySelector('video').play();

        var videos =document.querySelectorAll('video');
        for (var i=0;i<1;i++)
        {
            videos[i].playbackRate = 1.0;
        }
    </script>
    <script>
        /* https://stackoverflow.com/questions/21163756/html5-and-javascript-to-play-videos-only-when-visible */
        var videos = document.getElementsByTagName("video");

        function checkScroll() {
            var fraction = 0.5; // Play when 70% of the player is visible.

            for(var i = 0; i < 1; i++) {  // only apply to the first video

                var video = videos[i];

                var x = video.offsetLeft, y = video.offsetTop, w = video.offsetWidth, h = video.offsetHeight, r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }
        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Function to check if the user is on a mobile device
            function isMobileDevice() {
                return /Mobi|Android/i.test(navigator.userAgent);
            }
            // If the user is on a mobile device, disable autoplay
            if (isMobileDevice()) {
                const videos = document.querySelectorAll('video');
                videos.forEach(video => {
                    video.autoplay = false;
                    video.controls = true;
                });
            }
        });
    </script>
    <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
            type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
            crossorigin="anonymous"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
            type="text/javascript"></script>
  </body>

</html>
